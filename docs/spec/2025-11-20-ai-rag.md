# AI 接入与 RAG 架构解读说明

> 参考代码版本：2025-11-20，目录以 `D:/code/AgentFlowCreativeHub` 为根。

## 1. 端到端数据流概览

1. **API 入口**：`backend/api/setup.go` 将 `/api/agents`、`/api/workflows`、`/api/knowledge-bases` 等路由统一挂载在 JWT 中间件之后，所有请求都会带上 `tenant_id/user_id`。
2. **Handler 层**：如 `agents.NewAgentExecuteHandler`、`workflows.NewWorkflowExecuteHandler`、`knowledgeHandlers.NewSearchHandler` 负责鉴权校验与请求转译，并构造 `runtime.AgentInput`、`workflow.ExecuteRequest` 等内部结构。
3. **Service / Registry**：
   - Agent 执行走 `backend/internal/agent/runtime.Registry`：加载 AgentConfig、拼接 Prompt、注入上下文、按需调用工具/RAG。
   - Workflow 执行由 `backend/internal/workflow/executor` 接管，各节点再委托 Agent Registry 或工具执行器。
4. **AI Client Factory**：`backend/internal/ai/factory.go` 根据模型记录创建具体 `ModelClient`（OpenAI/Anthropic/Azure/Qwen/DeepSeek…），并按租户缓存实例。
5. **模型推理**：`aiinterface.ChatCompletionRequest` 统一承载 `Messages/Temperature/MaxTokens/Tools`，结果返回后在 Handler 层包装为 API 响应。
6. **可选工具链**：若模型响应中包含 `tool_calls`，`runtime.ToolHelper` 会并发调用 `backend/internal/tools` 中注册工具（HTTP、代码或内置），再把输出写回会话继续对话。
7. **可选 RAG**：启用 RAG 的 Agent 会在模型调用前通过 `ragHelper` 访问 `backend/internal/rag.RAGService`，将检索到的片段拼入 Prompt。

整体链路：`前端/自动化 → REST API → Handler → Service/Registry → AI Client → (工具执行) → (RAG 检索)`，同时结合审计、日志与指标中间件确保可观测性。

## 2. 模型接入与参数配置

### 2.1 模型结构

- `backend/internal/models/models.go` 中的 `Model` 记录包含：
  - **Provider / ModelIdentifier**：标记供应商与具体部署/系列。
  - **能力字段**：`ContextWindow`、`MaxTokens`、`SupportsStreaming`、`SupportsFunctionCalling`、`Features`（vision/jsonMode 等），供前端展示及运行时限制。
  - **协议配置**：`BaseURL`、`APIVersion`、`APIFormat`，支持自建代理或 Azure、Gemini 等异构协议。

### 2.2 客户端工厂

- `backend/internal/ai/factory.go`：
  - 优先使用模型记录的 `BaseURL`，否则按 `Provider` 填默认端点（OpenAI/Anthropic/Google/Qwen/DeepSeek 等）。
  - `resolveCredentials` 负责查找 API Key（模型记录 → 环境变量），确保多租户隔离。
  - Azure 特殊逻辑：`APIVersion` 会注入 `config.Extra["api_version"]`，`backend/internal/ai/azure/client.go` 使用 `openai.DefaultAzureConfig` 生成客户端。
  - 其他供应商（`openai`、`anthropic`、`qwen`、`deepseek`）位于 `backend/internal/ai/<provider>/client.go`，统一实现 `aiinterface.ModelClient` 接口。

### 2.3 参数继承链

1. **租户层**：`backend/internal/tenant/models.go` 中的 `TenantConfig` 保存 `DefaultModel`、`MaxTokensPerRequest`、`Temperature`、`DefaultEmbeddingModel` 等全局缺省值。
2. **Agent 层**：`backend/internal/agent/models.go` 为每个 Agent 记录 `Temperature`、`MaxTokens`、`KnowledgeBaseID`、`RAGTopK/RAGMinScore` 等细粒度参数。
3. **请求层**：`backend/api/handlers/agents/execute_handler.go` 接收用户输入后，可通过 `extraParams` 覆盖 `temperature/max_tokens/memory_mode/compress_threshold` 等运行时选项。
4. **运行时层**：`runtime.Registry` 将以上配置折叠成最终的 `ChatCompletionRequest`，并在必要时依据模型 `ContextWindow` + 租户 `MaxTokensPerRequest` 调整 Token 预算。

> “思考最大多少” 对应 `MaxTokens` 与 `ContextWindow` 的组合；若用户提供更大的历史记录，`ContextManager` 会优先截断以免超过模型窗口。

## 3. 工具调用与 HTTP API 集成

1. **工具模型**：`backend/internal/tools/models.go` 定义工具的 `Type`（`http_api`、`code_interpreter`、`builtin`）、参数 Schema、认证信息（API Key/Bearer/Basic）、`Timeout`、`MaxRetries`。
2. **注册/执行**：`backend/api/handlers/tools/tool_handler.go` 暴露注册、查询、执行 API。执行时将 `tenant_id/user_id` 注入输入，并封装为 `tools.ToolExecutionRequest`。
3. **执行层实现**：
   - 动态 HTTP：`backend/internal/tools/dynamic_http_tool.go` 按配置拼装请求、处理 JSON/文本响应。
   - 内置工具：位于 `backend/internal/tools/builtin/*`，例如 `http_api_tool.go`（简化版 HTTP 调度）、`text_statistics_tool.go`、`keyword_extractor_tool.go`。
4. **与模型联动**：`backend/internal/agent/runtime/tool_helper.go` 监听模型返回的 `ToolCalls`，使用 goroutine 并发执行并将结果 JSON 回写到对话，直至模型结束输出。

## 4. 会话上下文与自动压缩

1. **历史获取**：`backend/internal/agent/runtime/context_manager.go` 的 `GetHistory/EnrichInput` 支持：
   - `historyLimit`：硬性限制条数。
   - `maxTokens`：利用 `TrimHistoryByTokens` 和 `tiktoken` 估算 token，优先保留 System Prompt + 最近消息。
   - `memory_mode=summary`：会在历史前追加 `memory_summary`（若存在）。
2. **自动压缩**：`runtime.Registry` 在执行前读取 `compress_threshold`（默认 4000），超出则调用 `CompressHistory`：保留最近消息，并对旧对话生成摘要。
3. **持久化**：执行结束后由 `ContextManager.SaveInteraction` 将本轮问答写入 Redis/存储，确保下一轮可继续。

## 5. RAG 与嵌入模型

1. **配置项**：
   - Agent：`KnowledgeBaseID`、`RAGEnabled`、`RAGTopK`、`RAGMinScore`（`backend/internal/agent/models.go`）。
   - 租户：`DefaultEmbeddingModel`、`VectorSearchTopK`、`VectorSearchThreshold`（`backend/internal/tenant/models.go`）。
2. **知识库链路**：`backend/internal/rag/rag_service.go` + `backend/api/handlers/knowledge/*`：
   - 上传文档 → `Chunker`（默认 500 字符，支持 `chunkOverlap`）分句切块。
   - `OpenAIEmbeddingProvider`（`text-embedding-3-small`，1536 维）向量化，写入 `VectorStore`（`pgvector_store` 或 `qdrant_store`）。
   - `Search` 接口按 `TopK` 返回 `SearchResult`，同时记录 Prometheus 指标。
3. **执行增强**：如 `backend/internal/agent/runtime/analyzer_agent.go` 所示，在模型调用前使用 `ragHelper.EnrichWithKnowledge` 注入检索片段，提高答案准确性。
4. **调优建议**：
   - Chunk 500~800 字符 + 50~100 字符重叠，减少语义断裂。
   - 问答类任务 `TopK=5~8`、`RAGMinScore≥0.7`，若需覆盖面更大可提高 TopK 并在 Prompt 中要求引用多条证据。
   - 嵌入模型如需更高精度可改用 `text-embedding-3-large` 或其他 Provider，需同步调整 VectorStore 维度。

## 6. 配置与实践建议

| 场景 | 配置位置 | 建议值 / 要点 |
| --- | --- | --- |
| 模型 API Key / BaseURL | `.env` → `config/dev.yaml` → 模型记录 | 每个 Provider 独立配置；使用自建代理时在 `BaseURL` + `APIFormat=openai` 保持兼容。 |
| 默认温度 / Token | `tenant_config.temperature`、`tenant_config.maxTokensPerRequest` | 创作型 0.7~0.9，分析型 0.2~0.4；Token 预算需低于模型 `ContextWindow`。 |
| Agent 级参数 | `agent.AgentConfig` | 为不同 Agent 设置专属 `MaxTokens`、`RAGTopK`、`ExtraConfig`（例如 `{"memory_mode":"summary"}`）。 |
| 自动压缩阈值 | 请求 `extraParams.compress_threshold` | 默认 4000，可根据目标模型窗口（GPT-4o mini ~6k，Claude 3.5 ~8k）调整。 |
| 工具调用超时 | `tools.ToolsDefinition.Timeout` | 默认 30s，外部 API 慢时建议拆分为异步任务或提升超时并严格限流。 |
| RAG 检索参数 | Agent 配置 / `TenantConfig.VectorSearchTopK` | TopK 5~8，`VectorSearchThreshold` 0.7 起步，依据召回质量微调。 |

### 操作步骤速览

1. **新增模型**：调用 `/api/models` 或 `/api/models/discover/:provider` 创建模型记录，填写 `provider/modelIdentifier/baseUrl/apiVersion`。
2. **设置租户默认值**：`PUT /api/tenant/config` 更新默认模型、温度、MaxTokens 以及默认嵌入模型。
3. **配置 Agent**：`POST /api/agents` 绑定模型、系统 Prompt、RAG 选项及工具使用策略。
4. **注册工具 / API**：`POST /api/tools/register` 声明 `http_api` 工具（method/url/headers/参数 schema），在 Agent Prompt 中指示需调用的 Tool 名称。
5. **构建知识库**：`POST /api/knowledge-bases` 创建 KB → 上传文档 → 待状态 `completed` 后使用 `/search` 验证召回，再在 Agent 中启用 RAG。

借助以上说明可定位“API 接入、模型参数链、工具调用、上下文压缩、RAG/嵌入调优”的关键文件与配置，使后续扩展更易落地。
