package ai

import (
	"context"
	"os"
	"testing"
	"time"

	"backend/internal/cache"
	modelspkg "backend/internal/models"
)

// mockModelClient æ¨¡æ‹Ÿçš„AIå®¢æˆ·ç«¯
type mockModelClient struct {
	callCount int // è®°å½•å®é™…APIè°ƒç”¨æ¬¡æ•°
}

func (m *mockModelClient) ChatCompletion(ctx context.Context, req *ChatCompletionRequest) (*ChatCompletionResponse, error) {
	m.callCount++
	
	// æ¨¡æ‹ŸAPIå»¶è¿Ÿ
	time.Sleep(100 * time.Millisecond)
	
	return &ChatCompletionResponse{
		ID:      "mock-response-id",
		Model:   "gpt-3.5-turbo",
		Content: "This is a mock response for: " + req.Messages[0].Content,
		Usage: Usage{
			PromptTokens:     10,
			CompletionTokens: 20,
			TotalTokens:      30,
		},
	}, nil
}

func (m *mockModelClient) ChatCompletionStream(ctx context.Context, req *ChatCompletionRequest) (<-chan StreamChunk, <-chan error) {
	return nil, nil
}

func (m *mockModelClient) Embedding(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error) {
	return nil, nil
}

func (m *mockModelClient) Name() string {
	return "mock"
}

func (m *mockModelClient) Close() error {
	return nil
}

// TestLoggingClientCache æµ‹è¯•ç¼“å­˜åŠŸèƒ½
func TestLoggingClientCache(t *testing.T) {
	// åˆ›å»ºä¸´æ—¶æ•°æ®åº“æ–‡ä»¶
	tempDB := "./test_cache.db"
	defer os.Remove(tempDB)
	
	// åˆå§‹åŒ–ç¡¬ç›˜ç¼“å­˜
	diskCache, err := cache.NewDiskCache(tempDB, 1*time.Hour, 1)
	if err != nil {
		t.Fatalf("åˆ›å»ºDiskCacheå¤±è´¥: %v", err)
	}
	defer diskCache.Close()
	
	// åˆ›å»ºmockå®¢æˆ·ç«¯
	mockClient := &mockModelClient{}
	
	// åˆ›å»ºLoggingClientï¼ˆå¸¦ç¼“å­˜ï¼‰
	model := &modelspkg.Model{
		ID:               "test-model",
		Name:             "Test Model",
		Provider:         "openai",
		ModelIdentifier:  "gpt-3.5-turbo",
		InputCostPer1K:   0.001,
		OutputCostPer1K:  0.002,
	}
	
	loggingClient := NewLoggingClient(mockClient, nil, "tenant-1", "model-1", model, diskCache)
	
	// æ„å»ºæµ‹è¯•è¯·æ±‚ï¼ˆä½æ¸©åº¦ï¼Œå¯ç¼“å­˜ï¼‰
	req := &ChatCompletionRequest{
		Messages: []Message{
			{Role: "user", Content: "Hello, world!"},
		},
		Temperature: 0.1, // ä½æ¸©åº¦ï¼Œå¯ç”¨ç¼“å­˜
		MaxTokens:   100,
		TopP:        0.9,
	}
	
	ctx := context.Background()
	
	// ç¬¬ä¸€æ¬¡è°ƒç”¨ - åº”è¯¥å‘½ä¸­API
	t.Log("ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆåº”è¯¥è°ƒç”¨å®é™…APIï¼‰...")
	start1 := time.Now()
	resp1, err := loggingClient.ChatCompletion(ctx, req)
	latency1 := time.Since(start1).Milliseconds()
	
	if err != nil {
		t.Fatalf("ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥: %v", err)
	}
	if resp1 == nil {
		t.Fatal("å“åº”ä¸ºç©º")
	}
	if mockClient.callCount != 1 {
		t.Fatalf("æœŸæœ›APIè°ƒç”¨æ¬¡æ•°ä¸º1ï¼Œå®é™…ä¸º%d", mockClient.callCount)
	}
	t.Logf("âœ… ç¬¬ä¸€æ¬¡è°ƒç”¨æˆåŠŸ - å»¶è¿Ÿ: %dms, APIè°ƒç”¨æ¬¡æ•°: %d", latency1, mockClient.callCount)
	
	// ç­‰å¾…ç¼“å­˜å†™å…¥å®Œæˆï¼ˆå¼‚æ­¥å†™å…¥ï¼‰
	time.Sleep(200 * time.Millisecond)
	
	// ç¬¬äºŒæ¬¡è°ƒç”¨ - åº”è¯¥å‘½ä¸­ç¼“å­˜
	t.Log("ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰...")
	start2 := time.Now()
	resp2, err := loggingClient.ChatCompletion(ctx, req)
	latency2 := time.Since(start2).Milliseconds()
	
	if err != nil {
		t.Fatalf("ç¬¬äºŒæ¬¡è°ƒç”¨å¤±è´¥: %v", err)
	}
	if resp2 == nil {
		t.Fatal("ç¼“å­˜å“åº”ä¸ºç©º")
	}
	if mockClient.callCount != 1 {
		t.Fatalf("æœŸæœ›APIè°ƒç”¨æ¬¡æ•°ä»ä¸º1ï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰ï¼Œå®é™…ä¸º%d", mockClient.callCount)
	}
	
	// éªŒè¯ç¼“å­˜å“åº”ä¸åŸå§‹å“åº”ç›¸åŒ
	if resp2.Content != resp1.Content {
		t.Fatalf("ç¼“å­˜å“åº”å†…å®¹ä¸ä¸€è‡´: æœŸæœ› %s, å®é™… %s", resp1.Content, resp2.Content)
	}
	
	// ç¼“å­˜å‘½ä¸­åº”è¯¥æ˜¾è‘—å¿«äºç¬¬ä¸€æ¬¡è°ƒç”¨
	if latency2 >= latency1 {
		t.Logf("âš ï¸  ç¼“å­˜å»¶è¿Ÿ(%dms)æœªæ˜æ˜¾ä½äºAPIå»¶è¿Ÿ(%dms) - å¯èƒ½ç¼“å­˜æŸ¥è¯¢è¾ƒæ…¢", latency2, latency1)
	} else {
		t.Logf("âœ… ç¬¬äºŒæ¬¡è°ƒç”¨å‘½ä¸­ç¼“å­˜ - å»¶è¿Ÿ: %dms (æé€Ÿ%.1fx), APIè°ƒç”¨æ¬¡æ•°: %d",
			latency2, float64(latency1)/float64(latency2), mockClient.callCount)
	}
	
	// ç¬¬ä¸‰æ¬¡è°ƒç”¨ä¸åŒå†…å®¹ - åº”è¯¥æœªå‘½ä¸­ç¼“å­˜
	t.Log("ç¬¬ä¸‰æ¬¡è°ƒç”¨ï¼ˆä¸åŒå†…å®¹ï¼Œåº”è¯¥æœªå‘½ä¸­ç¼“å­˜ï¼‰...")
	req3 := &ChatCompletionRequest{
		Messages: []Message{
			{Role: "user", Content: "Different message"},
		},
		Temperature: 0.1,
		MaxTokens:   100,
		TopP:        0.9,
	}
	
	resp3, err := loggingClient.ChatCompletion(ctx, req3)
	if err != nil {
		t.Fatalf("ç¬¬ä¸‰æ¬¡è°ƒç”¨å¤±è´¥: %v", err)
	}
	if resp3 == nil {
		t.Fatal("ç¬¬ä¸‰æ¬¡å“åº”ä¸ºç©º")
	}
	if mockClient.callCount != 2 {
		t.Fatalf("æœŸæœ›APIè°ƒç”¨æ¬¡æ•°ä¸º2ï¼Œå®é™…ä¸º%d", mockClient.callCount)
	}
	t.Logf("âœ… ç¬¬ä¸‰æ¬¡è°ƒç”¨æˆåŠŸï¼ˆä¸åŒå†…å®¹ï¼‰ - APIè°ƒç”¨æ¬¡æ•°: %d", mockClient.callCount)
	
	// æµ‹è¯•é«˜æ¸©åº¦è¯·æ±‚ï¼ˆä¸åº”ä½¿ç”¨ç¼“å­˜ï¼‰
	t.Log("ç¬¬å››æ¬¡è°ƒç”¨ï¼ˆé«˜æ¸©åº¦ï¼Œä¸åº”ä½¿ç”¨ç¼“å­˜ï¼‰...")
	req4 := &ChatCompletionRequest{
		Messages: []Message{
			{Role: "user", Content: "Hello, world!"}, // ç›¸åŒå†…å®¹
		},
		Temperature: 0.8, // é«˜æ¸©åº¦ï¼Œä¸å¯ç”¨ç¼“å­˜
		MaxTokens:   100,
		TopP:        0.9,
	}
	
	resp4, err := loggingClient.ChatCompletion(ctx, req4)
	if err != nil {
		t.Fatalf("ç¬¬å››æ¬¡è°ƒç”¨å¤±è´¥: %v", err)
	}
	if resp4 == nil {
		t.Fatal("ç¬¬å››æ¬¡å“åº”ä¸ºç©º")
	}
	if mockClient.callCount != 3 {
		t.Fatalf("æœŸæœ›APIè°ƒç”¨æ¬¡æ•°ä¸º3ï¼ˆé«˜æ¸©åº¦ä¸ç¼“å­˜ï¼‰ï¼Œå®é™…ä¸º%d", mockClient.callCount)
	}
	t.Logf("âœ… ç¬¬å››æ¬¡è°ƒç”¨æˆåŠŸï¼ˆé«˜æ¸©åº¦ï¼‰ - APIè°ƒç”¨æ¬¡æ•°: %d", mockClient.callCount)
	
	t.Log("ğŸ‰ æ‰€æœ‰ç¼“å­˜æµ‹è¯•é€šè¿‡ï¼")
}
